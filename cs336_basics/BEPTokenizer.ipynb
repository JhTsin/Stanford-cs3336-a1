{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "312c752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "from typing import BinaryIO\n",
    "from typing import Iterable, Iterator\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Process, Queue\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436b20c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO, \n",
    "    desired_num_chunks: int, \n",
    "    split_special_token: bytes\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), (\n",
    "        \"Must represent special token as a bytestring\"\n",
    "    )\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "    return sorted(set(chunk_boundaries))\n",
    "\n",
    "def split_by_special_tokens(text: str, special_tokens: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split on the special tokens\n",
    "    example: \n",
    "        text = \"Hello world! <|endoftext|> Great!\" \n",
    "        special_tokens = \"<|endoftext|>\"\n",
    "        result = ['Hello world! ', '<|endoftext|>', ' Great!']\n",
    "    \"\"\"\n",
    "    special_tokens_sorted = sorted(special_tokens, key=lambda x: -len(x))\n",
    "    if not special_tokens_sorted:\n",
    "        parts = [text]\n",
    "    else:\n",
    "        pattern = \"|\".join(re.escape(tok) for tok in special_tokens_sorted)\n",
    "        parts = re.split('(' + pattern + ')', text)\n",
    "\n",
    "    return parts\n",
    "\n",
    "def pretokenize(text: str, special_tokens: list[str], drop_special_token: bool = True) -> list[bytes]:\n",
    "    \"\"\"\n",
    "    Seperating text into pretokens\n",
    "    Special tokens are independent pretokens\n",
    "    \"\"\"\n",
    "    parts = split_by_special_tokens(text, special_tokens)\n",
    "    #parts内容: ['Hello, how ', '<|endoftext|><|endoftext|>', ' are you?', '<|endoftext|>', '']\n",
    "    PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    tokens_list = []\n",
    "    for part in parts:\n",
    "        if part in special_tokens:\n",
    "            if not drop_special_token:  # Keep special tokens, otherwise ignore\n",
    "                spec_tok_bytes = part.encode('utf-8')\n",
    "                tokens_list.append([spec_tok_bytes])\n",
    "        else:\n",
    "            #str_tokens = re.findall(PAT, part)  #re.finditer(PAT, part)更好\n",
    "            #part_tokens = [s.encode('utf-8') for s in str_tokens]\n",
    "            # 更好的方式\n",
    "            str_tokens = re.finditer(PAT, part)  # 返回Match对象迭代器\n",
    "            part_tokens = [match.group().encode('utf-8') for match in str_tokens]\n",
    "\n",
    "            tokens_list.append(part_tokens)\n",
    "    #tokens_list内容: [[b'Hello', b',', b' how', b' '], [b'<|endoftext|><|endoftext|>'], [b' are', b' you', b'?'], [b'<|endoftext|>'], []]\n",
    "    tokens = [token for part_tokens in tokens_list for token in part_tokens]    #flatten token_list\n",
    "    return tokens\n",
    "\n",
    "def worker(text: str, special_tokens: list[str], q: Queue):\n",
    "    try:\n",
    "        print(\"[worker] start\")\n",
    "        pretokens = pretokenize(text, special_tokens)\n",
    "        print(\"[worker] done pretokenize, len:\", len(pretokens))\n",
    "        q.put(pretokens)\n",
    "    except Exception as e:\n",
    "        print(\"[worker] error:\", e)\n",
    "        q.put([])\n",
    "\n",
    "def train_bpe(\n",
    "    input_path: str | os.PathLike,\n",
    "    vocab_size: int,\n",
    "    special_tokens: list[str],\n",
    "    **kwargs,\n",
    ") -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    \"\"\"Given the path to an input corpus, run train a BPE tokenizer and\n",
    "    output its vocabulary and merges.\n",
    "\n",
    "    Args:\n",
    "        input_path (str | os.PathLike): Path to BPE tokenizer training data.\n",
    "        vocab_size (int): Total number of items in the tokenizer's vocabulary (including special tokens).\n",
    "        special_tokens (list[str]): A list of string special tokens to be added to the tokenizer vocabulary.\n",
    "            These strings will never be split into multiple tokens, and will always be\n",
    "            kept as a single token. If these special tokens occur in the `input_path`,\n",
    "            they are treated as any other string.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "            vocab:\n",
    "                The trained tokenizer vocabulary, a mapping from int (token ID in the vocabulary)\n",
    "                to bytes (token bytes)\n",
    "            merges:\n",
    "                BPE merges. Each list item is a tuple of bytes (<token1>, <token2>),\n",
    "                representing that <token1> was merged with <token2>.\n",
    "                Merges are ordered by order of creation.\n",
    "    \"\"\"\n",
    "    print(\"train_bpe: start\")\n",
    "    special_tokens = special_tokens or []\n",
    "    num_merges = max(vocab_size - len(special_tokens) - 256, 0)\n",
    "\n",
    "    # Initialize vocab\n",
    "    vocab = {}\n",
    "    vocab = {x:bytes([x]) for x in range(0,256)}\n",
    "    for i, token in enumerate(special_tokens):\n",
    "        vocab[256+i] = token.encode(\"utf-8\")\n",
    "    merges = []\n",
    "\n",
    "    # Chunk the text file\n",
    "    num_processes = 4\n",
    "    chunk_list = []\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        boundaries = find_chunk_boundaries(f, num_processes, \"<|endoftext|>\".encode(\"utf-8\"))\n",
    "\n",
    "        for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "            f.seek(start)\n",
    "            chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "            chunk_list.append(chunk)\n",
    "\n",
    "    print(\"train_bpe: after chunking\")\n",
    "    print(\"chunk_list len:\", len(chunk_list))\n",
    "    for i, chunk in enumerate(chunk_list):\n",
    "        print(f\"chunk {i} len:\", len(chunk))\n",
    "    # # 只用前1个chunk\n",
    "    #chunk_list = chunk_list[:1]\n",
    "    # Parallelizing pretokenization\n",
    "    pretokens_list = []\n",
    "    processes = []\n",
    "    q = Queue()\n",
    "    for chunk in chunk_list:\n",
    "        print(\"[main] starting worker\")\n",
    "        p = Process(target=worker, args=(chunk, special_tokens, q))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for i, p in enumerate(processes):\n",
    "        print(f\"[main] waiting for q.get() from worker {i}\")\n",
    "        pretokens = q.get()\n",
    "        print(f\"[main] got result from worker {i}, len={len(pretokens)}\")\n",
    "        pretokens_list.append(pretokens)\n",
    "\n",
    "    for i, p in enumerate(processes):\n",
    "        print(f\"[main] joining worker {i}\")\n",
    "        p.join()\n",
    "        print(f\"[main] worker {i} joined\")\n",
    "\n",
    "    pretokens = [token for tokens in pretokens_list for token in tokens]\n",
    "    print(\"train_bpe: after pretokenization\")\n",
    "    print(\"total pretokens:\", len(pretokens))\n",
    "    print(\"first pretoken len:\", len(pretokens[0]))\n",
    "    # Merging\n",
    "    counts = defaultdict(int)   #统计相邻对的频率\n",
    "    index_dict = defaultdict(set)  # Store pretoken location for each pair记录块索引\n",
    "\n",
    "    for j, pretoken in tqdm(enumerate(pretokens), total=len(pretokens), desc=\"Counting pairs\"):\n",
    "        for index1, index2 in zip(pretoken, pretoken[1:]): #对pretoken遍历相邻对\n",
    "            counts[index1, index2] += 1\n",
    "            index_dict[index1, index2].add(j)\n",
    "\n",
    "    for i in tqdm(range(num_merges), desc=\"Merging BPE pairs\"):\n",
    "        # Prefer lexicographically greater pair 频率相同时 字典序更大的优先 代表更丰富的语义信息\n",
    "        # Example: max([(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), (\"BA\", \"A\")]) = ('BA', 'A')\n",
    "        max_pair = max(\n",
    "            counts.items(),\n",
    "            key=lambda x: (\n",
    "                x[1],  \n",
    "                vocab[x[0][0]].decode(\"utf-8\", errors=\"ignore\"),\n",
    "                vocab[x[0][1]].decode(\"utf-8\", errors=\"ignore\")\n",
    "            )\n",
    "        )[0]\n",
    "\n",
    "        index1, index2 = max_pair\n",
    "\n",
    "        new_index = 256 + len(special_tokens) + i\n",
    "\n",
    "        vocab[new_index] = vocab[index1] + vocab[index2]\n",
    "        merges.append((vocab[index1], vocab[index2]))\n",
    "\n",
    "        merge(counts, index_dict, pretokens, max_pair, new_index)\n",
    "        #进行左右侧计数更，指标更新，pretokens更新\n",
    "    return (vocab, merges)\n",
    "\n",
    "def merge(counts: dict[tuple[int, int], int], index_dict: dict[tuple[int, int],set[int]], pretokens: list[list[int]], max_pair: tuple[int, int], new_index: int):\n",
    "    \"\"\"Merge the pairs with highest frequency and update counts, index_dict\"\"\"\n",
    "    index_set = index_dict[max_pair]    #获取需要处理的分块​\n",
    "\n",
    "    for i in index_set:\n",
    "        pretoken = pretokens[i]\n",
    "        new_pretoken = []\n",
    "\n",
    "        pos_list = []   # Store positions of max_pair for each new pretoken after merge\n",
    "        pos = 0\n",
    "        j = 0\n",
    "\n",
    "        # Replace max_pair with new_index in each pretoken\n",
    "        while j < len(pretoken):\n",
    "            if (j < len(pretoken)-1) and ((pretoken[j], pretoken[j+1]) == max_pair):\n",
    "                new_pretoken.append(new_index)\n",
    "                pos_list.append(pos)\n",
    "                j += 2\n",
    "            else:\n",
    "                new_pretoken.append(pretoken[j])\n",
    "                j += 1\n",
    "            pos += 1    #新的pretoken里面合并的位置的索引\n",
    "\n",
    "        # Update counts and index_dict\n",
    "        for pos in pos_list:\n",
    "            counts[max_pair] -= 1\n",
    "\n",
    "            if pos > 0: #合并位置不是开头第一位，更新左侧相邻对\n",
    "                if new_pretoken[pos-1] == new_index: #若左侧也是合并对，即连续合并情况\n",
    "                    counts[(max_pair[1], max_pair[0])] -= 1 #BA减少一次\n",
    "                else:   #左侧是普通符号\n",
    "                    counts[(new_pretoken[pos-1], max_pair[0])] -= 1 #左侧与合并第一位减少一次\n",
    "\n",
    "                counts[(new_pretoken[pos-1], new_pretoken[pos])] += 1   #左侧与合并体增加一次\n",
    "                index_dict[(new_pretoken[pos-1], new_pretoken[pos])].add(i) #记录新对的位置，也就是上面一行的对\n",
    "\n",
    "            if pos < len(new_pretoken)-1:   #更新右侧相邻对\n",
    "                if new_pretoken[pos+1] == new_index:\n",
    "                    counts[(max_pair[1], max_pair[0])] -= 1     \n",
    "                else:\n",
    "                    counts[(max_pair[1], new_pretoken[pos+1])] -= 1\n",
    "\n",
    "                counts[(new_pretoken[pos], new_pretoken[pos+1])] += 1\n",
    "                index_dict[(new_pretoken[pos], new_pretoken[pos+1])].add(i)\n",
    "\n",
    "        pretokens[i] = new_pretoken #每个块里面更新pretoken\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a9b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, vocab: dict[int, bytes], merges: list[tuple[bytes, bytes]], special_tokens: list[str]| None = None):\n",
    "        self.vocab = vocab\n",
    "        self.merges = merges\n",
    "        self.special_tokens = special_tokens or []\n",
    "\n",
    "    @classmethod\n",
    "    def from_files(cls, vocab_filepath: str, merges_filepath: str, special_tokens: list[str] | None = None):\n",
    "        \"\"\"Class method that constructs and return a Tokenizer from a serialized vocabulary and list of merges\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def encode(self, text:str) -> list[int]:\n",
    "        \"\"\"Encode an input text into a sequence of token IDs.\"\"\"\n",
    "\n",
    "        vocab_reversed = {v: k for k, v in self.vocab.items()}  # int: bytes-->bytes: int\n",
    "        byte_pretokens = pretokenize(text, self.special_tokens, drop_special_token=False)   # list[bytes]\n",
    "        byte_special_tokens = [token.encode('utf-8') for token in self.special_tokens]\n",
    "        pretokens = []  # list[list[int]]\n",
    "\n",
    "        # Convert pretokens from bytes to list[int] by vocab\n",
    "        for i, pretoken in enumerate(byte_pretokens):\n",
    "\n",
    "            new_pretoken = []\n",
    "\n",
    "            if pretoken in byte_special_tokens:\n",
    "                index = vocab_reversed[pretoken]\n",
    "                new_pretoken.append(index)\n",
    "            else:\n",
    "                for b in pretoken:  #普通token按照字节处理\n",
    "                    index = vocab_reversed[bytes([b])]\n",
    "                    new_pretoken.append(index)\n",
    "\n",
    "            pretokens.append(new_pretoken)\n",
    "\n",
    "        # Merge  三重循环\n",
    "        #首先对pretokens中的每个pretoken进行merge\n",
    "        #其次对合并规则表进行逐项匹配\n",
    "        #对于每一个merge，遍历pretoken若其中有相邻的merge则合并，这样避免了跨单词合并\n",
    "        for i, pretoken in enumerate(pretokens):\n",
    "            for merge in self.merges:   #merges: list[tuple[bytes, bytes]],\n",
    "                new_pretoken = []\n",
    "                new_index = vocab_reversed[merge[0] + merge[1]]\n",
    "                j = 0\n",
    "                while j < len(pretoken):\n",
    "                    if (j < len(pretoken)-1) and ((self.vocab[pretoken[j]], self.vocab[pretoken[j+1]]) == merge):\n",
    "                        new_pretoken.append(new_index)\n",
    "                        j += 2\n",
    "                    else:\n",
    "                        new_pretoken.append(pretoken[j])\n",
    "                        j += 1\n",
    "                        #new_pretoken保存的是合并一个merge后每个pretoken的id串\n",
    "                pretoken = new_pretoken\n",
    "\n",
    "            pretokens[i] = pretoken\n",
    "            #pretokens[i]保存的是合并所有merge后每个pretoken的id串，然后对i循环所有的pretokens\n",
    "        tokens = [token for pretoken in pretokens for token in pretoken] \n",
    "        return tokens\n",
    "\n",
    "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "        \"\"\"Given an iterable of strings (e.g., a Python file handle), \n",
    "        return a generator that lazily yields token IDs. \n",
    "        This is required for memory-eﬀicient tokenization of large files \n",
    "        that we cannot directly load into memory.\n",
    "        \"\"\"\n",
    "        for line in iterable:\n",
    "            for idx in self.encode(line):\n",
    "                yield idx\n",
    "\n",
    "\n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        \"\"\"Decode a sequence of token IDs into text.\"\"\"\n",
    "        tokens = bytes()\n",
    "        vocab_size = len(self.vocab)\n",
    "        replacement_char = \"\\uFFFD\"\n",
    "\n",
    "        for token_id in ids:\n",
    "            if token_id < vocab_size:\n",
    "                token = self.vocab[token_id]    # vocab: int: bytes\n",
    "            else:\n",
    "                token = bytes(replacement_char, encoding='utf-8')   # Replace tokens with Unicode replacement characters if index out of bounds\n",
    "\n",
    "            tokens += token\n",
    "        decoded = tokens.decode(encoding='utf-8', errors='replace')\n",
    "\n",
    "        return decoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1882ad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    import tiktoken\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    test_string = \"Hello, how <|endoftext|><|endoftext|> are you?<|endoftext|>\"\n",
    "    ids = tokenizer.encode(test_string, allowed_special={\"<|endoftext|><|endoftext|>\", \"<|endoftext|>\"})\n",
    "    decoded = [tokenizer.decode([x]) for x in ids]\n",
    "    print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "57064264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_bpe: start\n",
      "train_bpe: after chunking\n",
      "chunk_list len: 4\n",
      "chunk 0 len: 5623437\n",
      "chunk 1 len: 5624335\n",
      "chunk 2 len: 5622697\n",
      "chunk 3 len: 5622918\n",
      "[main] starting worker\n",
      "[main] starting worker\n",
      "[worker] start\n",
      "[main] starting worker\n",
      "[worker] start\n",
      "[main] starting worker\n",
      "[worker] start\n",
      "[worker] start\n",
      "[main] waiting for q.get() from worker 0\n",
      "[worker] done pretokenize, len: 1355040\n",
      "[worker] done pretokenize, len: 1354266\n",
      "[worker] done pretokenize, len: [worker] done pretokenize, len:1355347 \n",
      "1354348\n",
      "[main] got result from worker 0, len=1355040\n",
      "[main] waiting for q.get() from worker 1\n",
      "[main] got result from worker 1, len=1354266\n",
      "[main] waiting for q.get() from worker 2\n",
      "[main] got result from worker 2, len=1354348\n",
      "[main] waiting for q.get() from worker 3\n",
      "[main] got result from worker 3, len=1355347\n",
      "[main] joining worker 0\n",
      "[main] worker 0 joined\n",
      "[main] joining worker 1\n",
      "[main] worker 1 joined\n",
      "[main] joining worker 2\n",
      "[main] worker 2 joined\n",
      "[main] joining worker 3\n",
      "[main] worker 3 joined\n",
      "train_bpe: after pretokenization\n",
      "total pretokens: 5419001\n",
      "first pretoken len: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting pairs: 100%|██████████| 5419001/5419001 [00:05<00:00, 910438.82it/s]\n",
      "Merging BPE pairs: 100%|██████████| 72/72 [00:36<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "pretokens内容 (ID转换后，合并前):\n",
      "pretokens数量: 9\n",
      "pretoken[0]: [72, 101, 108, 108, 111]\n",
      "  对应字节串: [b'H', b'e', b'l', b'l', b'o']\n",
      "pretoken[1]: [44]\n",
      "  对应字节串: [b',']\n",
      "pretoken[2]: [32, 104, 111, 119]\n",
      "  对应字节串: [b' ', b'h', b'o', b'w']\n",
      "pretoken[3]: [32]\n",
      "  对应字节串: [b' ']\n",
      "pretoken[4]: [257]\n",
      "  对应字节串: [b'<|endoftext|><|endoftext|>']\n",
      "pretoken[5]: [32, 97, 114, 101]\n",
      "  对应字节串: [b' ', b'a', b'r', b'e']\n",
      "pretoken[6]: [32, 121, 111, 117]\n",
      "  对应字节串: [b' ', b'y', b'o', b'u']\n",
      "pretoken[7]: [63]\n",
      "  对应字节串: [b'?']\n",
      "pretoken[8]: [256]\n",
      "  对应字节串: [b'<|endoftext|>']\n",
      "============================================================\n",
      "encoded: [72, 101, 294, 111, 44, 269, 319, 32, 257, 260, 274, 32, 121, 276, 63, 256]\n",
      "decoded: ['H', 'e', 'll', 'o', ',', ' h', 'ow', ' ', '<|endoftext|><|endoftext|>', ' a', 're', ' ', 'y', 'ou', '?', '<|endoftext|>']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../data/TinyStoriesV2-GPT4-valid.txt\"\n",
    "vocab_size = 330  # 临时减少\n",
    "special_tokens = [\"<|endoftext|>\", \"<|endoftext|><|endoftext|>\"]\n",
    "\n",
    "vocab, merges = train_bpe(file_path, vocab_size, special_tokens)\n",
    "\n",
    "tokenizer = BPETokenizer(vocab, merges, special_tokens)\n",
    "test_string = \"Hello, how <|endoftext|><|endoftext|> are you?<|endoftext|>\"\n",
    "encoded = tokenizer.encode(test_string)\n",
    "print(\"encoded:\",encoded)\n",
    "decoded = [tokenizer.decode([x]) for x in encoded]\n",
    "print(\"decoded:\", decoded)\n",
    "\n",
    "print(test_string == ''.join(decoded))\n",
    "\n",
    "    # print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2eff7f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, b'\\x00'), (1, b'\\x01'), (2, b'\\x02'), (3, b'\\x03'), (4, b'\\x04'), (5, b'\\x05'), (6, b'\\x06'), (7, b'\\x07'), (8, b'\\x08'), (9, b'\\t'), (10, b'\\n'), (11, b'\\x0b'), (12, b'\\x0c'), (13, b'\\r'), (14, b'\\x0e'), (15, b'\\x0f'), (16, b'\\x10'), (17, b'\\x11'), (18, b'\\x12'), (19, b'\\x13'), (20, b'\\x14'), (21, b'\\x15'), (22, b'\\x16'), (23, b'\\x17'), (24, b'\\x18'), (25, b'\\x19'), (26, b'\\x1a'), (27, b'\\x1b'), (28, b'\\x1c'), (29, b'\\x1d'), (30, b'\\x1e'), (31, b'\\x1f'), (32, b' '), (33, b'!'), (34, b'\"'), (35, b'#'), (36, b'$'), (37, b'%'), (38, b'&'), (39, b\"'\"), (40, b'('), (41, b')'), (42, b'*'), (43, b'+'), (44, b','), (45, b'-'), (46, b'.'), (47, b'/'), (48, b'0'), (49, b'1')]\n",
      "[(b' ', b'p'), (b'a', b'y'), (b' ', b'm'), (b'e', b'r'), (b' wa', b's'), (b' T', b'he'), (b'o', b'm'), (b' ', b'he'), (b'i', b's'), (b' ', b'n'), (b'i', b'm'), (b'a', b'r'), (b'o', b'n'), (b' s', b'a'), (b'l', b'l'), (b'i', b'd'), (b' h', b'a'), (b' ', b'g'), (b'a', b't'), (b' ', b'S'), (b'in', b'g'), (b'o', b't'), (b'e', b'n'), (b'a', b'n'), (b'l', b'e'), (b'o', b'r'), (b'i', b'r'), (b'a', b'm'), (b'e', b't'), (b' ', b'H'), (b' ', b'it'), (b' t', b'h'), (b'i', b'g'), (b' The', b'y'), (b' p', b'l'), (b' ', b'in'), (b'i', b'l'), (b' H', b'e'), (b' ', b'\"'), (b'o', b'w'), (b'v', b'er'), (b'r', b'i'), (b' ', b'u'), (b'u', b't'), (b' pl', b'ay'), (b'it', b'h'), (b' sa', b'id'), (b' b', b'e'), (b' d', b'ay'), (b' w', b'ith')]\n"
     ]
    }
   ],
   "source": [
    "print(list(vocab.items())[:50])\n",
    "print(list(merges)[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "480e35cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{b'\\x00': 0, b'\\x01': 1, b'\\x02': 2, b'\\x03': 3, b'\\x04': 4, b'\\x05': 5, b'\\x06': 6, b'\\x07': 7, b'\\x08': 8, b'\\t': 9, b'\\n': 10, b'\\x0b': 11, b'\\x0c': 12, b'\\r': 13, b'\\x0e': 14, b'\\x0f': 15, b'\\x10': 16, b'\\x11': 17, b'\\x12': 18, b'\\x13': 19, b'\\x14': 20, b'\\x15': 21, b'\\x16': 22, b'\\x17': 23, b'\\x18': 24, b'\\x19': 25, b'\\x1a': 26, b'\\x1b': 27, b'\\x1c': 28, b'\\x1d': 29, b'\\x1e': 30, b'\\x1f': 31, b' ': 32, b'!': 33, b'\"': 34, b'#': 35, b'$': 36, b'%': 37, b'&': 38, b\"'\": 39, b'(': 40, b')': 41, b'*': 42, b'+': 43, b',': 44, b'-': 45, b'.': 46, b'/': 47, b'0': 48, b'1': 49, b'2': 50, b'3': 51, b'4': 52, b'5': 53, b'6': 54, b'7': 55, b'8': 56, b'9': 57, b':': 58, b';': 59, b'<': 60, b'=': 61, b'>': 62, b'?': 63, b'@': 64, b'A': 65, b'B': 66, b'C': 67, b'D': 68, b'E': 69, b'F': 70, b'G': 71, b'H': 72, b'I': 73, b'J': 74, b'K': 75, b'L': 76, b'M': 77, b'N': 78, b'O': 79, b'P': 80, b'Q': 81, b'R': 82, b'S': 83, b'T': 84, b'U': 85, b'V': 86, b'W': 87, b'X': 88, b'Y': 89, b'Z': 90, b'[': 91, b'\\\\': 92, b']': 93, b'^': 94, b'_': 95, b'`': 96, b'a': 97, b'b': 98, b'c': 99, b'd': 100, b'e': 101, b'f': 102, b'g': 103, b'h': 104, b'i': 105, b'j': 106, b'k': 107, b'l': 108, b'm': 109, b'n': 110, b'o': 111, b'p': 112, b'q': 113, b'r': 114, b's': 115, b't': 116, b'u': 117, b'v': 118, b'w': 119, b'x': 120, b'y': 121, b'z': 122, b'{': 123, b'|': 124, b'}': 125, b'~': 126, b'\\x7f': 127, b'\\x80': 128, b'\\x81': 129, b'\\x82': 130, b'\\x83': 131, b'\\x84': 132, b'\\x85': 133, b'\\x86': 134, b'\\x87': 135, b'\\x88': 136, b'\\x89': 137, b'\\x8a': 138, b'\\x8b': 139, b'\\x8c': 140, b'\\x8d': 141, b'\\x8e': 142, b'\\x8f': 143, b'\\x90': 144, b'\\x91': 145, b'\\x92': 146, b'\\x93': 147, b'\\x94': 148, b'\\x95': 149, b'\\x96': 150, b'\\x97': 151, b'\\x98': 152, b'\\x99': 153, b'\\x9a': 154, b'\\x9b': 155, b'\\x9c': 156, b'\\x9d': 157, b'\\x9e': 158, b'\\x9f': 159, b'\\xa0': 160, b'\\xa1': 161, b'\\xa2': 162, b'\\xa3': 163, b'\\xa4': 164, b'\\xa5': 165, b'\\xa6': 166, b'\\xa7': 167, b'\\xa8': 168, b'\\xa9': 169, b'\\xaa': 170, b'\\xab': 171, b'\\xac': 172, b'\\xad': 173, b'\\xae': 174, b'\\xaf': 175, b'\\xb0': 176, b'\\xb1': 177, b'\\xb2': 178, b'\\xb3': 179, b'\\xb4': 180, b'\\xb5': 181, b'\\xb6': 182, b'\\xb7': 183, b'\\xb8': 184, b'\\xb9': 185, b'\\xba': 186, b'\\xbb': 187, b'\\xbc': 188, b'\\xbd': 189, b'\\xbe': 190, b'\\xbf': 191, b'\\xc0': 192, b'\\xc1': 193, b'\\xc2': 194, b'\\xc3': 195, b'\\xc4': 196, b'\\xc5': 197, b'\\xc6': 198, b'\\xc7': 199, b'\\xc8': 200, b'\\xc9': 201, b'\\xca': 202, b'\\xcb': 203, b'\\xcc': 204, b'\\xcd': 205, b'\\xce': 206, b'\\xcf': 207, b'\\xd0': 208, b'\\xd1': 209, b'\\xd2': 210, b'\\xd3': 211, b'\\xd4': 212, b'\\xd5': 213, b'\\xd6': 214, b'\\xd7': 215, b'\\xd8': 216, b'\\xd9': 217, b'\\xda': 218, b'\\xdb': 219, b'\\xdc': 220, b'\\xdd': 221, b'\\xde': 222, b'\\xdf': 223, b'\\xe0': 224, b'\\xe1': 225, b'\\xe2': 226, b'\\xe3': 227, b'\\xe4': 228, b'\\xe5': 229, b'\\xe6': 230, b'\\xe7': 231, b'\\xe8': 232, b'\\xe9': 233, b'\\xea': 234, b'\\xeb': 235, b'\\xec': 236, b'\\xed': 237, b'\\xee': 238, b'\\xef': 239, b'\\xf0': 240, b'\\xf1': 241, b'\\xf2': 242, b'\\xf3': 243, b'\\xf4': 244, b'\\xf5': 245, b'\\xf6': 246, b'\\xf7': 247, b'\\xf8': 248, b'\\xf9': 249, b'\\xfa': 250, b'\\xfb': 251, b'\\xfc': 252, b'\\xfd': 253, b'\\xfe': 254, b'\\xff': 255, b'<|endoftext|>': 256, b'<|endoftext|><|endoftext|>': 257, b' t': 258, b'he': 259, b' a': 260, b' s': 261, b' w': 262, b' the': 263, b'nd': 264, b'ed': 265, b' b': 266, b' to': 267, b' and': 268, b' h': 269, b' f': 270, b' T': 271, b'in': 272, b' wa': 273, b're': 274, b'it': 275, b'ou': 276, b' l': 277, b' d': 278, b' c': 279, b' p': 280, b'ay': 281, b' m': 282, b'er': 283, b' was': 284, b' The': 285, b'om': 286, b' he': 287, b'is': 288, b' n': 289, b'im': 290, b'ar': 291, b'on': 292, b' sa': 293, b'll': 294, b'id': 295, b' ha': 296, b' g': 297, b'at': 298, b' S': 299, b'ing': 300, b'ot': 301, b'en': 302, b'an': 303, b'le': 304, b'or': 305, b'ir': 306, b'am': 307, b'et': 308, b' H': 309, b' it': 310, b' th': 311, b'ig': 312, b' They': 313, b' pl': 314, b' in': 315, b'il': 316, b' He': 317, b' \"': 318, b'ow': 319, b'ver': 320, b'ri': 321, b' u': 322, b'ut': 323, b' play': 324, b'ith': 325, b' said': 326, b' be': 327, b' day': 328, b' with': 329}\n"
     ]
    }
   ],
   "source": [
    "vocab_reversed = {v: k for k, v in vocab.items()}  # bytes: int\n",
    "print(vocab_reversed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
